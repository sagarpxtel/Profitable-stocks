{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final_Project.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g44R_VKRIV6-"
      },
      "source": [
        "**Step 0:** Setting up environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o__XXpTjGa-B"
      },
      "source": [
        "!apt-get update -qq > /dev/null\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://downloads.apache.org/spark/spark-2.4.7/spark-2.4.7-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.7-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "!pip install yahoo_fin --upgrade"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdSbJepaI45h"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.7-bin-hadoop2.7\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "spark_conf = SparkConf()\\\n",
        "  .setAppName(\"YourTest\")\\\n",
        "  .setMaster(\"local[*]\")\n",
        "\n",
        "sc = SparkContext.getOrCreate(spark_conf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kY6f9nG3Gs5V"
      },
      "source": [
        "import yahoo_fin.stock_info as si"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j37ce-GIHC6v"
      },
      "source": [
        "**Step 1:** Gathered all the required data for companies in our index of interest (S&P500) from yahoo_fin API into a spark RDD data structure."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HA9XoKVG0z0"
      },
      "source": [
        "\n",
        "tickers = si.tickers_sp500() # replace with any basket of stocks\n",
        "\n",
        "def get_info(ticker):\n",
        "  df1 = si.get_stats(ticker)\n",
        "  df2 = si.get_analysts_info(ticker)['Growth Estimates']\n",
        "\n",
        "\n",
        "  eps = float(df1[df1['Attribute'] == 'Diluted EPS (ttm)'].iloc[0][1])\n",
        "  growth_rate = float(df2[df2['Growth Estimates'] == 'Next 5 Years (per annum)'].iloc[0][1][:-1])\n",
        "  if eps == float(\"nan\") or growth_rate == float(\"nan\"):\n",
        "    return 0, 0, 0\n",
        "  PE_ratio = growth_rate * 2       # Can also be obtained from api, then take minimum of two, but it shouldn't be none!\n",
        "  return eps, growth_rate, PE_ratio\n",
        "\n",
        "\n",
        "rdd = sc.parallelize(tickers).cache()\n",
        "stock_info = rdd.map(lambda x: (x, get_info(x))).filter(lambda x: x == None) # Format -- (Ticker, (info))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dR70kLfAbis7"
      },
      "source": [
        "Step 2: Looked 10 years into the future with expected growth rate and calculated future stock price."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAbGyr56Y8u1"
      },
      "source": [
        "\n",
        "YEARS = 10 # can be changed to look further into future\n",
        "\n",
        "def get_future_growth(eps, growth_rate):\n",
        "  for i in range(YEARS):\n",
        "    eps += eps*(growth_rate/100)\n",
        "  return eps\n",
        "\n",
        "future_growth = stock_info.map(lambda x: (x[0], x[1], get_future_growth(x[1][0], x[1][1]))).map(lambda x: (x[0], x[1], x[2]*x[1][2])) # Format -- (Ticker, (info), future_price)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvIBeMQmfjIo"
      },
      "source": [
        "Step 3: Calculated discounted stock price today, by deducting expected returns and applying margin of safety.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdM4j-A-ZExO"
      },
      "source": [
        "MIN_RATE_RETURN = 15        # Return expected\n",
        "MARGIN_OF_SAFETY = 50       # Increase for conservative investment, lower for riskier\n",
        "\n",
        "def get_current_valuation(value):\n",
        "  for i in range(YEARS):\n",
        "    value = value/( 1 + (MIN_RATE_RETURN/100) )\n",
        "  return value\n",
        "\n",
        "\n",
        "discounted_price_today = future_growth.map(lambda x: (x[0], x[1], get_current_valuation(x[2]))) # Format -- (Ticker, (info), intrinsic_price)\n",
        "apply_margin_of_safety = discounted_price_today.map(lambda x: (x[0], x[2]*(MARGIN_OF_SAFETY/100))) # Format -- (Ticker,  intrinsic_margin_price)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOe8U57q2ijl"
      },
      "source": [
        "Step 4: Get current valuation percentage and assign valuation score. Then, categorize into following baskets -- \n",
        "\n",
        "- (+3) Extremely Under-valued: 75% or more\n",
        "\n",
        "- (+2) Moderately Under-valued: 25% to 75%\n",
        "\n",
        "- (+1) Somewhat Under-valued: 0 to 25%\n",
        "\n",
        "- (-1) Somewhat Over-valued: 0 to -25%\n",
        "\n",
        "- (-2) Moderately Over-valued: -25% to -75%\n",
        "\n",
        "- (-3) Extremely Over-valued: -75% or more\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukxz9GfN3aL9"
      },
      "source": [
        "\n",
        "value_stocks = apply_margin_of_safety.map(lambda x: (x[0], x[1], si.get_live_price(x[0]))) # Format -- (Ticker,  intrinsic_price, live_price)\n",
        "valuation = value_stocks.map(lambda x: (x[0], (100 * (x[1] - x[2])) / x[2])) # Format -- (Ticker, Valuation %) -- [Postive % == Undervalued ; Negative % == Overvalued]\n",
        "\n",
        "def categorize_stocks(info):\n",
        "  ticker, x = info\n",
        "  if x > 0 and x <= 25:\n",
        "    return (1, ticker)\n",
        "  elif x > 25 and x <= 75:\n",
        "    return (2, ticker)\n",
        "  elif x > 75:\n",
        "    return (3, ticker)\n",
        "  elif x < 0 and x >= -25:\n",
        "    return (-1, ticker)\n",
        "  elif x < -25 and x >= -75:\n",
        "    return (-2, ticker)\n",
        "  elif x < -75:\n",
        "    return (-3, ticker)\n",
        "  else:\n",
        "    return (0, x) # for 'None' cases\n",
        "\n",
        "\n",
        "scores = valuation.map(categorize_stocks) # Format -- (valution_score, Ticker)\n",
        "# groups = scores.groupByKey() #Not working..\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14pOIKrP96h2"
      },
      "source": [
        "print(scores.take(70))\n",
        "\n",
        "# # print(si.tickers_sp500()) #BKR\n",
        "\n",
        "# df1 = si.get_stats('bkr')\n",
        "# eps = float(df1[df1['Attribute'] == 'Diluted EPS (ttm)'].iloc[0][1])\n",
        "\n",
        "# print(type(eps))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}